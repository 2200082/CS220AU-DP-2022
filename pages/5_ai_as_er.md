# Is Artifical Intelligence itself an Existential Risk?
In my last blog post about AI in general, I explored the different levels of AI. The third level, Artificial Superintelligence, was also introduced. In the literature, this was described as the state in which AI "would be capable of outperforming humans" ([levity.ai](https://levity.ai/blog/general-ai-vs-narrow-ai)). Therefore, in this post I would like to think about which scenarios there would be and to what extent these would allow AI to be classified as an existential risk.<br>
To do this, I would first like to learn more about artificial superintelligence.
## Definition
> „Artificial superintelligence (ASI) is a form of AI that is capable of surpassing human intelligence by manifesting cognitive skills and developing thinking skills of its own. Also known as super AI, artificial superintelligence is considered the most advanced, powerful, and intelligent type of AI that transcends the intelligence of some of the brightest minds, such as Albert Einstein” ([spiceworks.com](https://www.spiceworks.com/tech/artificial-intelligence/articles/super-artificial-intelligence/)).

According to spiceworks.com, ASI is able to acquire and develop all these human skills:<br><br>

<p align="center">
  <img src="/assets/img/asi.jpg">
</p>

If you take a closer look at when such a state of Artificial Super Intelligence could possibly occur, opinions differ. The majority of opinions, however, are found in a time span of 20 to 30 years: Louis Rosenberg - computer scientist and entrepreneur - speaks of 2030, Ray Kurzweil - computer scientist - of 2045 and Jurgen Schmidhuber - co-founder of NNAISENSE, a Swiss AI startup - of 2050. However, no one can predict with certainty whether and when such Artificial Superintelligence will exist ([levity.ai](https://levity.ai/blog/general-ai-vs-narrow-ai)).
## Characteristics
In the Journal of Artificial Intelligence Research 70 (2021), the following characteristics of ASI can be recognised ([spiceworks.com](https://www.spiceworks.com/tech/artificial-intelligence/articles/super-artificial-intelligence/)):
1. Because ASI will continually improve and grow more intelligent, it will be one of the best and perhaps the **final innovations** that humans will ever need to create.
2. The emergence of superintelligence will hasten **technological advancement** in a variety of areas, including academia, space exploration, pharmaceutical discovery and development, and many others.
3. ASI may further grow and produce sophisticated kinds of superintelligence that might even make it possible to **duplicate human minds**.
4. ASI may eventually result in the **technological singularity**.

## Potential Advantages
...


## Risks
While some may emphasise the emerging opportunities, there are also those who are more critical of the whole development and point out the risks. Even though I have now discovered the characteristics of ASI, I am now more critical of ASI than I was at the beginning. <br>
The next step will be to look at the exact risks that ASI entails: <br><br>
Referring to the recent study published in the Journal of Artificial Intelligence Research in January 2021, it was found that "it would be almost impossible for humans to contain super AIs ([spiceworks.com](https://www.spiceworks.com/tech/artificial-intelligence/articles/super-artificial-intelligence/)). Let us now take a closer look at the risks associated with ASI in order to validate this very definitive statement.

<p align="center">
  <img src="/assets/img/asi_threats.jpg">
</p>



## Navigation
