# Is Artifical Intelligence itself an Existential Risk?
In my last blog post about AI in general, I explored the different levels of AI. The third level, Artificial Superintelligence, was also introduced. In the literature, this was described as the state in which AI "would be capable of outperforming humans" ([levity.ai](https://levity.ai/blog/general-ai-vs-narrow-ai)). Therefore, in this post I would like to think about which scenarios there would be and to what extent these would allow AI to be classified as an existential risk.<br>
To do this, I would first like to learn more about artificial superintelligence.
## Definition
> „Artificial superintelligence (ASI) is a form of AI that is capable of surpassing human intelligence by manifesting cognitive skills and developing thinking skills of its own. Also known as super AI, artificial superintelligence is considered the most advanced, powerful, and intelligent type of AI that transcends the intelligence of some of the brightest minds, such as Albert Einstein” ([spiceworks.com](https://www.spiceworks.com/tech/artificial-intelligence/articles/super-artificial-intelligence/)).

According to spiceworks.com, ASI is able to acquire and develop all these human skills:<br><br>

<p align="center">
  <img src="/assets/img/asi.jpg">
</p>

If you take a closer look at when such a state of Artificial Super Intelligence could possibly occur, opinions differ. The majority of opinions, however, are found in a time span of 20 to 30 years: Louis Rosenberg - computer scientist and entrepreneur - speaks of 2030, Ray Kurzweil - computer scientist - of 2045 and Jurgen Schmidhuber - co-founder of NNAISENSE, a Swiss AI startup - of 2050. However, no one can predict with certainty whether and when such Artificial Superintelligence will exist ([levity.ai](https://levity.ai/blog/general-ai-vs-narrow-ai)).
## Characteristics
...
## Risks
...
## Navigation
